{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae66d3eb",
   "metadata": {},
   "source": [
    "\n",
    "# Lap-to-Lap Race State Transformer\n",
    "\n",
    "This notebook prepares the FastF1-derived lap dataset, tensorizes each race into driver/lap sequences, and trains a multi-head transformer that predicts the next-lap state (lap time, pit flag, tyre compound, retirements) for every car simultaneously. The model follows the architecture outlined in the design notes: per-car tokens enriched with driver/team/track embeddings, autoregressive supervision with teacher forcing, and multi-task losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17281229",
   "metadata": {},
   "source": [
    "\n",
    "## Environment setup\n",
    "Install the Python packages required for data processing and model training. PyTorch provides the transformer layers; pandas/numpy handle the preprocessing workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a88f1e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install --quiet torch pandas numpy scikit-learn tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833d97b",
   "metadata": {},
   "source": [
    "\n",
    "## Imports and configuration\n",
    "The configuration block centralises hyperparameters so it is easy to tweak sequence length, batch size, train/validation splits, or debugging limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c7d4041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb2704eac90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    dataset_path: Path\n",
    "    seq_len: int = 8\n",
    "    max_drivers: int = 20\n",
    "    min_laps_per_session: int = 10\n",
    "    train_years: Tuple[int, ...] = (2018, 2019, 2020, 2021)\n",
    "    val_years: Tuple[int, ...] = (2023,)\n",
    "    test_years: Tuple[int, ...] = (2024, 2025)\n",
    "    batch_size: int = 2\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    #max_train_steps_per_epoch: Optional[int] = 50\n",
    "    #max_val_steps: Optional[int] = 25\n",
    "    max_train_steps_per_epoch: Optional[int] = None\n",
    "    max_val_steps: Optional[int] = None\n",
    "    debug_num_sessions: Optional[int] = None\n",
    "    seed: int = 42\n",
    "\n",
    "CONFIG = TrainerConfig(\n",
    "    dataset_path=Path('fastf1_lap_dataset2.csv'),\n",
    ")\n",
    "\n",
    "np.random.seed(CONFIG.seed)\n",
    "torch.manual_seed(CONFIG.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e906b03",
   "metadata": {},
   "source": [
    "\n",
    "## Data loading helpers\n",
    "We load the lap-level dataset, clean missing values, derive pit flags, and compute per-track scalers for lap times and gaps. Weather features use global scalers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "631a72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_lap_dataframe(csv_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=['session_key', 'driver_id', 'lap_number'])\n",
    "    df['lap_number'] = df['lap_number'].astype(int)\n",
    "    df['current_position'] = df['current_position'].fillna(df['lap_number'])\n",
    "    df['grid_position'] = df['grid_position'].fillna(df.groupby(['session_key', 'driver_id'])['grid_position'].transform('first'))\n",
    "    df['grid_position'] = df['grid_position'].fillna(df['current_position'])\n",
    "    df['laps_on_current_tyre'] = df['laps_on_current_tyre'].fillna(1).clip(lower=1)\n",
    "    # simple pit detection based on tyre age reset\n",
    "    df = df.sort_values(['session_key', 'driver_id', 'lap_number'])\n",
    "    df['pit_flag'] = False\n",
    "    for (session_key, driver_id), group in df.groupby(['session_key', 'driver_id']):\n",
    "        laps = group['laps_on_current_tyre'].values\n",
    "        pit_flags = np.zeros_like(laps, dtype=bool)\n",
    "        prev = laps[0]\n",
    "        for idx in range(1, len(laps)):\n",
    "            pit_flags[idx] = laps[idx] <= prev - 1\n",
    "            prev = laps[idx]\n",
    "        df.loc[group.index, 'pit_flag'] = pit_flags\n",
    "    # fill lap_time gaps per session\n",
    "    df['lap_time_s'] = df.groupby('session_key')['lap_time_s'].transform(lambda s: s.fillna(s.median()))\n",
    "    df['lap_time_s'] = df['lap_time_s'].fillna(df['lap_time_s'].median())\n",
    "    df['gap_to_leader_s'] = df['gap_to_leader_s'].fillna(0)\n",
    "    df['gap_to_ahead_s'] = df['gap_to_ahead_s'].fillna(0)\n",
    "    df['tyre_compound'] = df['tyre_compound'].fillna('UNKNOWN')\n",
    "    df['track_temperature'] = df['track_temperature'].fillna(df['track_temperature'].median())\n",
    "    df['air_temperature'] = df['air_temperature'].fillna(df['air_temperature'].median())\n",
    "    df['humidity'] = df['humidity'].fillna(df['humidity'].median())\n",
    "    df['wind_speed'] = df['wind_speed'].fillna(df['wind_speed'].median())\n",
    "    df['wind_direction'] = df['wind_direction'].fillna(0)\n",
    "    df['rainfall'] = df['rainfall'].fillna(False)\n",
    "    df['has_rain'] = df['has_rain'].fillna(False)\n",
    "    df['safety_car_this_lap'] = df['safety_car_this_lap'].fillna(False)\n",
    "    df['virtual_sc_this_lap'] = df['virtual_sc_this_lap'].fillna(False)\n",
    "    df['drs_enabled'] = df['drs_enabled'].fillna(False)\n",
    "    df['current_position'] = df['current_position'].clip(lower=1, upper=CONFIG.max_drivers)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def compute_track_scalers(df: pd.DataFrame) -> Dict[str, Dict[str, Tuple[float, float]]]:\n",
    "    stats = {}\n",
    "    global_defaults = {\n",
    "        'lap_time_s': (df['lap_time_s'].mean(), df['lap_time_s'].std() or 1.0),\n",
    "        'gap_to_leader_s': (df['gap_to_leader_s'].mean(), df['gap_to_leader_s'].std() or 1.0),\n",
    "        'gap_to_ahead_s': (df['gap_to_ahead_s'].mean(), df['gap_to_ahead_s'].std() or 1.0),\n",
    "    }\n",
    "    for circuit_id, grp in df.groupby('circuit_id'):\n",
    "        stats[circuit_id] = {}\n",
    "        for col in ['lap_time_s', 'gap_to_leader_s', 'gap_to_ahead_s']:\n",
    "            series = grp[col].dropna()\n",
    "            if series.empty:\n",
    "                stats[circuit_id][col] = global_defaults[col]\n",
    "            else:\n",
    "                mean = float(series.mean())\n",
    "                std = float(series.std()) or 1.0\n",
    "                stats[circuit_id][col] = (mean, std)\n",
    "    return stats\n",
    "\n",
    "def compute_weather_scaler(df: pd.DataFrame) -> Dict[str, Tuple[float, float]]:\n",
    "    scalers = {}\n",
    "    for col in ['track_temperature', 'air_temperature', 'humidity', 'wind_speed']:\n",
    "        series = df[col].astype(float)\n",
    "        scalers[col] = (float(series.mean()), float(series.std() or 1.0))\n",
    "    return scalers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b689e3",
   "metadata": {},
   "source": [
    "\n",
    "## Vocabulary builders\n",
    "We map every categorical field to contiguous integer IDs so the model can use embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "680c9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class LapVocabulary:\n",
    "    driver_to_idx: Dict[str, int]\n",
    "    team_to_idx: Dict[str, int]\n",
    "    compound_to_idx: Dict[str, int]\n",
    "    track_to_idx: Dict[str, int]\n",
    "\n",
    "    @property\n",
    "    def num_drivers(self) -> int:\n",
    "        return len(self.driver_to_idx) + 1  # include padding\n",
    "\n",
    "    @property\n",
    "    def num_teams(self) -> int:\n",
    "        return len(self.team_to_idx) + 1\n",
    "\n",
    "    @property\n",
    "    def num_compounds(self) -> int:\n",
    "        return len(self.compound_to_idx) + 1\n",
    "\n",
    "    @property\n",
    "    def num_tracks(self) -> int:\n",
    "        return len(self.track_to_idx) + 1\n",
    "\n",
    "\n",
    "def build_vocab(df: pd.DataFrame) -> LapVocabulary:\n",
    "    drivers = sorted(df['driver_id'].dropna().unique().tolist())\n",
    "    teams = sorted(df['team_id'].dropna().unique().tolist())\n",
    "    compounds = sorted(df['tyre_compound'].dropna().unique().tolist())\n",
    "    tracks = sorted(df['circuit_id'].dropna().unique().tolist())\n",
    "    driver_to_idx = {drv: idx + 1 for idx, drv in enumerate(drivers)}\n",
    "    team_to_idx = {team: idx + 1 for idx, team in enumerate(teams)}\n",
    "    compound_to_idx = {comp: idx + 1 for idx, comp in enumerate(compounds)}\n",
    "    track_to_idx = {trk: idx + 1 for idx, trk in enumerate(tracks)}\n",
    "    compound_to_idx.setdefault('UNKNOWN', len(compound_to_idx) + 1)\n",
    "    return LapVocabulary(driver_to_idx, team_to_idx, compound_to_idx, track_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50495a8c",
   "metadata": {},
   "source": [
    "\n",
    "## Session tensor builder\n",
    "Convert each race (session) into padded tensors. Each session stores dynamic features for shape `[laps, max_drivers, F_dyn]`, categorical tokens, and targets aligned for next-step prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06c6410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class SessionTensor:\n",
    "    session_key: str\n",
    "    year: int\n",
    "    track_token: int\n",
    "    dynamic_numeric: np.ndarray  # [T, max_drivers, F_dyn]\n",
    "    global_numeric: np.ndarray   # [T, F_global]\n",
    "    rank_tokens: np.ndarray      # [T, max_drivers]\n",
    "    compound_tokens: np.ndarray  # [T, max_drivers]\n",
    "    alive_mask: np.ndarray       # [T, max_drivers]\n",
    "    pit_flags: np.ndarray        # [T, max_drivers]\n",
    "    lap_time_norm: np.ndarray    # [T, max_drivers]\n",
    "    driver_tokens: np.ndarray    # [max_drivers]\n",
    "    team_tokens: np.ndarray      # [max_drivers]\n",
    "    static_numeric: np.ndarray   # [max_drivers, F_static]\n",
    "\n",
    "\n",
    "class SessionTensorBuilder:\n",
    "    def __init__(self, cfg: TrainerConfig, vocab: LapVocabulary, track_scalers, weather_scaler):\n",
    "        self.cfg = cfg\n",
    "        self.vocab = vocab\n",
    "        self.track_scalers = track_scalers\n",
    "        self.weather_scaler = weather_scaler\n",
    "\n",
    "    def _zscore(self, value, mean, std):\n",
    "        if pd.isna(value):\n",
    "            return 0.0\n",
    "        return float((value - mean) / std)\n",
    "\n",
    "    def build(self, df: pd.DataFrame) -> List[SessionTensor]:\n",
    "        sessions: List[SessionTensor] = []\n",
    "        grouped = df.groupby('session_key', sort=False)\n",
    "        for session_idx, (session_key, sdf) in enumerate(tqdm(grouped, desc='Sessions')):\n",
    "            if self.cfg.debug_num_sessions and session_idx >= self.cfg.debug_num_sessions:\n",
    "                break\n",
    "            sdf = sdf.sort_values(['lap_number', 'driver_id']).copy()\n",
    "            max_lap = int(sdf['lap_number'].max())\n",
    "            if max_lap < self.cfg.min_laps_per_session:\n",
    "                continue\n",
    "            drivers = sorted(sdf['driver_id'].unique().tolist())\n",
    "            if len(drivers) > self.cfg.max_drivers:\n",
    "                drivers = drivers[: self.cfg.max_drivers]\n",
    "            driver_to_slot = {drv: idx for idx, drv in enumerate(drivers)}\n",
    "            slots = self.cfg.max_drivers\n",
    "            dyn_features = 12\n",
    "            dynamic = np.zeros((max_lap, slots, dyn_features), dtype=np.float32)\n",
    "            rank_tokens = np.zeros((max_lap, slots), dtype=np.int64)\n",
    "            compound_tokens = np.zeros((max_lap, slots), dtype=np.int64)\n",
    "            alive = np.zeros((max_lap, slots), dtype=np.float32)\n",
    "            pit_flags = np.zeros((max_lap, slots), dtype=np.float32)\n",
    "            lap_time_norm = np.zeros((max_lap, slots), dtype=np.float32)\n",
    "            global_feats = np.zeros((max_lap, 8), dtype=np.float32)\n",
    "            driver_tokens = np.zeros((slots,), dtype=np.int64)\n",
    "            team_tokens = np.zeros((slots,), dtype=np.int64)\n",
    "            static_numeric = np.zeros((slots, 2), dtype=np.float32)\n",
    "            circuit_id = sdf['circuit_id'].iloc[0]\n",
    "            track_stats = self.track_scalers.get(circuit_id, self.track_scalers[next(iter(self.track_scalers))])\n",
    "            track_token = self.vocab.track_to_idx.get(circuit_id, 0)\n",
    "            total_laps = int(sdf['total_race_laps'].dropna().max() or max_lap)\n",
    "            total_laps = max(total_laps, max_lap)\n",
    "            for driver_id, driver_rows in sdf.groupby('driver_id'):\n",
    "                if driver_id not in driver_to_slot:\n",
    "                    continue\n",
    "                slot = driver_to_slot[driver_id]\n",
    "                driver_tokens[slot] = self.vocab.driver_to_idx.get(driver_id, 0)\n",
    "                team_name = driver_rows['team_id'].iloc[0]\n",
    "                team_tokens[slot] = self.vocab.team_to_idx.get(team_name, 0)\n",
    "                static_numeric[slot, 0] = float(driver_rows['grid_position'].iloc[0] / self.cfg.max_drivers)\n",
    "                static_numeric[slot, 1] = float(driver_rows['year'].iloc[0] - 2018) / 10.0\n",
    "                for _, row in driver_rows.iterrows():\n",
    "                    lap_idx = int(row['lap_number']) - 1\n",
    "                    dynamic[lap_idx, slot, 0] = (float(row['current_position']) - 1) / (self.cfg.max_drivers - 1)\n",
    "                    dynamic[lap_idx, slot, 1] = float(row['grid_position']) / self.cfg.max_drivers\n",
    "                    lap_mean, lap_std = track_stats['lap_time_s']\n",
    "                    lap_norm = self._zscore(row['lap_time_s'], lap_mean, lap_std)\n",
    "                    dynamic[lap_idx, slot, 2] = lap_norm\n",
    "                    dynamic[lap_idx, slot, 3] = self._zscore(row.get('lap_time_prev1', row['lap_time_s']), lap_mean, lap_std)\n",
    "                    dynamic[lap_idx, slot, 4] = self._zscore(row.get('lap_time_prev2', row['lap_time_s']), lap_mean, lap_std)\n",
    "                    gap_mean, gap_std = track_stats['gap_to_leader_s']\n",
    "                    ahead_mean, ahead_std = track_stats['gap_to_ahead_s']\n",
    "                    dynamic[lap_idx, slot, 5] = self._zscore(np.log1p(row['gap_to_leader_s']), np.log1p(gap_mean), gap_std or 1.0)\n",
    "                    dynamic[lap_idx, slot, 6] = self._zscore(np.log1p(row['gap_to_ahead_s']), np.log1p(ahead_mean), ahead_std or 1.0)\n",
    "                    dynamic[lap_idx, slot, 7] = float(row['laps_on_current_tyre']) / 50.0\n",
    "                    dynamic[lap_idx, slot, 8] = float(row['pit_flag'])\n",
    "                    dynamic[lap_idx, slot, 9] = float(row['drs_enabled'])\n",
    "                    dynamic[lap_idx, slot,10] = float(row['safety_car_this_lap'])\n",
    "                    dynamic[lap_idx, slot,11] = float(row['virtual_sc_this_lap'])\n",
    "                    compound_tokens[lap_idx, slot] = self.vocab.compound_to_idx.get(row['tyre_compound'], 0)\n",
    "                    rank_tokens[lap_idx, slot] = int(row['current_position']) - 1\n",
    "                    lap_time_norm[lap_idx, slot] = lap_norm\n",
    "                    pit_flags[lap_idx, slot] = float(row['pit_flag'])\n",
    "                    alive[lap_idx, slot] = 1.0\n",
    "            for lap_idx in range(max_lap):\n",
    "                lap_no = lap_idx + 1\n",
    "                frac = lap_no / total_laps\n",
    "                remaining = (total_laps - lap_no) / total_laps\n",
    "                global_feats[lap_idx, 0] = frac\n",
    "                global_feats[lap_idx, 1] = remaining\n",
    "                track_temp_mu, track_temp_std = self.weather_scaler['track_temperature']\n",
    "                air_mu, air_std = self.weather_scaler['air_temperature']\n",
    "                hum_mu, hum_std = self.weather_scaler['humidity']\n",
    "                wind_mu, wind_std = self.weather_scaler['wind_speed']\n",
    "                rows = sdf[sdf['lap_number'] == lap_no]\n",
    "                if rows.empty:\n",
    "                    continue\n",
    "                row = rows.iloc[0]\n",
    "                global_feats[lap_idx, 2] = self._zscore(row['track_temperature'], track_temp_mu, track_temp_std)\n",
    "                global_feats[lap_idx, 3] = self._zscore(row['air_temperature'], air_mu, air_std)\n",
    "                global_feats[lap_idx, 4] = self._zscore(row['humidity'], hum_mu, hum_std)\n",
    "                global_feats[lap_idx, 5] = self._zscore(row['wind_speed'], wind_mu, wind_std)\n",
    "                global_feats[lap_idx, 6] = float(row['has_rain'])\n",
    "                global_feats[lap_idx, 7] = float(row['pressure'] if not pd.isna(row['pressure']) else 0.0)\n",
    "            sessions.append(SessionTensor(\n",
    "                session_key=session_key,\n",
    "                year=int(sdf['year'].iloc[0]),\n",
    "                track_token=track_token,\n",
    "                dynamic_numeric=dynamic,\n",
    "                global_numeric=global_feats,\n",
    "                rank_tokens=rank_tokens,\n",
    "                compound_tokens=compound_tokens,\n",
    "                alive_mask=alive,\n",
    "                pit_flags=pit_flags,\n",
    "                lap_time_norm=lap_time_norm,\n",
    "                driver_tokens=driver_tokens,\n",
    "                team_tokens=team_tokens,\n",
    "                static_numeric=static_numeric,\n",
    "            ))\n",
    "        return sessions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6bffe",
   "metadata": {},
   "source": [
    "\n",
    "## Build session tensors\n",
    "Load the CSV, create scalers/vocabulary, and construct tensors for every race.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52b0f8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions: 100%|██████████| 168/168 [00:10<00:00, 16.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sessions tensorized: 167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_df = load_lap_dataframe(CONFIG.dataset_path)\n",
    "# derive lagged lap times for dynamic context\n",
    "raw_df['lap_time_prev1'] = raw_df.groupby(['session_key', 'driver_id'])['lap_time_s'].shift(1)\n",
    "raw_df['lap_time_prev2'] = raw_df.groupby(['session_key', 'driver_id'])['lap_time_s'].shift(2)\n",
    "raw_df['lap_time_prev1'] = raw_df['lap_time_prev1'].fillna(raw_df['lap_time_s'])\n",
    "raw_df['lap_time_prev2'] = raw_df['lap_time_prev2'].fillna(raw_df['lap_time_s'])\n",
    "\n",
    "track_scalers = compute_track_scalers(raw_df)\n",
    "weather_scaler = compute_weather_scaler(raw_df)\n",
    "vocab = build_vocab(raw_df)\n",
    "\n",
    "builder = SessionTensorBuilder(CONFIG, vocab, track_scalers, weather_scaler)\n",
    "session_tensors = builder.build(raw_df)\n",
    "print(f\"Total sessions tensorized: {len(session_tensors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa8378",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset and DataLoader\n",
    "Create sequence samples with teacher forcing. Each sample consists of `seq_len` laps and predicts lap `t+1` targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ac78cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4256 | Val: 1216 | Test: 2525\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LapSequenceDataset(Dataset):\n",
    "    def __init__(self, sessions: List[SessionTensor], cfg: TrainerConfig, split_years: Tuple[int, ...]):\n",
    "        self.sessions = [s for s in sessions if s.year in split_years]\n",
    "        self.cfg = cfg\n",
    "        self.indices = []  # (session_idx, lap_start)\n",
    "        for sess_idx, sess in enumerate(self.sessions):\n",
    "            T = sess.dynamic_numeric.shape[0]\n",
    "            max_start = T - cfg.seq_len - 1\n",
    "            if max_start < 0:\n",
    "                continue\n",
    "            for start in range(max_start + 1):\n",
    "                self.indices.append((sess_idx, start))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sess_idx, start = self.indices[idx]\n",
    "        sess = self.sessions[sess_idx]\n",
    "        seq_slice = slice(start, start + self.cfg.seq_len)\n",
    "        next_slice = slice(start + 1, start + self.cfg.seq_len + 1)\n",
    "        dynamic = torch.from_numpy(sess.dynamic_numeric[seq_slice]).float()\n",
    "        global_feats = torch.from_numpy(sess.global_numeric[seq_slice]).float()\n",
    "        compound_tokens = torch.from_numpy(sess.compound_tokens[seq_slice]).long()\n",
    "        rank_tokens = torch.from_numpy(sess.rank_tokens[seq_slice]).long()\n",
    "        alive_curr = torch.from_numpy(sess.alive_mask[seq_slice]).float()\n",
    "        alive_next = torch.from_numpy(sess.alive_mask[next_slice]).float()\n",
    "        lap_time_target = torch.from_numpy(sess.lap_time_norm[next_slice]).float()\n",
    "        pit_target = torch.from_numpy(sess.pit_flags[next_slice]).float()\n",
    "        compound_next = torch.from_numpy(sess.compound_tokens[next_slice]).long()\n",
    "        dnf_target = ((alive_curr == 1.0) & (alive_next == 0.0)).float()\n",
    "        return {\n",
    "            'dynamic': dynamic,\n",
    "            'global': global_feats,\n",
    "            'compound_tokens': compound_tokens,\n",
    "            'rank_tokens': rank_tokens,\n",
    "            'alive_curr': alive_curr,\n",
    "            'alive_next': alive_next,\n",
    "            'driver_tokens': torch.from_numpy(sess.driver_tokens).long(),\n",
    "            'team_tokens': torch.from_numpy(sess.team_tokens).long(),\n",
    "            'static_numeric': torch.from_numpy(sess.static_numeric).float(),\n",
    "            'track_token': torch.tensor(sess.track_token).long(),\n",
    "            'targets': {\n",
    "                'lap_time': lap_time_target,\n",
    "                'pit': pit_target,\n",
    "                'compound': compound_next,\n",
    "                'dnf': dnf_target,\n",
    "            },\n",
    "        }\n",
    "\n",
    "def collate_batch(batch):\n",
    "    out = {}\n",
    "    for key in ['dynamic', 'global', 'compound_tokens', 'rank_tokens', 'alive_curr', 'alive_next']:\n",
    "        out[key] = torch.stack([item[key] for item in batch], dim=0)\n",
    "    out['driver_tokens'] = torch.stack([item['driver_tokens'] for item in batch], dim=0)\n",
    "    out['team_tokens'] = torch.stack([item['team_tokens'] for item in batch], dim=0)\n",
    "    out['static_numeric'] = torch.stack([item['static_numeric'] for item in batch], dim=0)\n",
    "    out['track_token'] = torch.stack([item['track_token'] for item in batch], dim=0)\n",
    "    targets = {}\n",
    "    for tgt_key in batch[0]['targets'].keys():\n",
    "        targets[tgt_key] = torch.stack([item['targets'][tgt_key] for item in batch], dim=0)\n",
    "    out['targets'] = targets\n",
    "    return out\n",
    "\n",
    "train_dataset = LapSequenceDataset(session_tensors, CONFIG, CONFIG.train_years)\n",
    "val_dataset = LapSequenceDataset(session_tensors, CONFIG, CONFIG.val_years)\n",
    "test_dataset = LapSequenceDataset(session_tensors, CONFIG, CONFIG.test_years)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.batch_size, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a874634",
   "metadata": {},
   "source": [
    "\n",
    "## Model definition\n",
    "The transformer tokenizes each driver per lap. Static driver/team embeddings and global lap features are concatenated with the dynamic scalars. Multi-task heads predict the next lap time, pit decision, tyre compound, and retirement probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf5026e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LapStateTransformer(nn.Module):\n",
    "    def __init__(self, cfg: TrainerConfig, vocab: LapVocabulary, dyn_dim: int, global_dim: int, static_dim: int, rank_vocab: int = 20):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        d_model = 256\n",
    "        self.driver_emb = nn.Embedding(vocab.num_drivers + 1, 32, padding_idx=0)\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        d_model = 256\n",
    "        self.driver_emb = nn.Embedding(vocab.num_drivers + 1, 32, padding_idx=0)\n",
    "        self.team_emb = nn.Embedding(vocab.num_teams + 1, 16, padding_idx=0)\n",
    "        self.compound_emb = nn.Embedding(vocab.num_compounds + 1, 8, padding_idx=0)\n",
    "        self.rank_emb = nn.Embedding(rank_vocab + 1, 8, padding_idx=0)\n",
    "        self.track_emb = nn.Embedding(vocab.num_tracks + 1, 16, padding_idx=0)\n",
    "        self.static_proj = nn.Linear(static_dim + 32 + 16, 64)\n",
    "        self.global_proj = nn.Linear(global_dim + 16, 64)\n",
    "        self.token_proj = nn.Linear(dyn_dim + 8 + 8 + 64 + 64, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dim_feedforward=1024, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        self.time_head = nn.Sequential(nn.Linear(d_model, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "        self.rank_delta_head = nn.Sequential(nn.Linear(d_model, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        B, T, N, F_dyn = batch['dynamic'].shape\n",
    "        dyn = batch['dynamic'].to(TORCH_DEVICE)\n",
    "        rank_tokens = batch['rank_tokens'].to(TORCH_DEVICE)\n",
    "        compound_tokens = batch['compound_tokens'].to(TORCH_DEVICE)\n",
    "        driver_tokens = batch['driver_tokens'].to(TORCH_DEVICE)\n",
    "        team_tokens = batch['team_tokens'].to(TORCH_DEVICE)\n",
    "        static_numeric = batch['static_numeric'].to(TORCH_DEVICE)\n",
    "        global_feats = batch['global'].to(TORCH_DEVICE)\n",
    "        track_tokens = batch['track_token'].to(TORCH_DEVICE)\n",
    "        alive_mask = batch['alive_curr'].to(TORCH_DEVICE)\n",
    "\n",
    "        driver_emb = self.driver_emb(driver_tokens)\n",
    "        team_emb = self.team_emb(team_tokens)\n",
    "        static_cat = torch.cat([static_numeric, driver_emb, team_emb], dim=-1)\n",
    "        static_feat = self.static_proj(static_cat)\n",
    "        static_feat = static_feat.unsqueeze(1).expand(-1, T, -1, -1)\n",
    "\n",
    "        track_emb = self.track_emb(track_tokens).unsqueeze(1).expand(-1, T, -1)\n",
    "        global_cat = torch.cat([global_feats, track_emb], dim=-1)\n",
    "        global_feat = self.global_proj(global_cat)\n",
    "        global_feat = global_feat.unsqueeze(2).expand(-1, -1, N, -1)\n",
    "\n",
    "        rank_emb = self.rank_emb(rank_tokens)\n",
    "        compound_emb = self.compound_emb(compound_tokens)\n",
    "\n",
    "        x = torch.cat([dyn, rank_emb, compound_emb, static_feat, global_feat], dim=-1)\n",
    "        x = self.token_proj(x)\n",
    "        x = x.view(B * T, N, -1)\n",
    "        key_padding_mask = ~(alive_mask.view(B * T, N).bool())\n",
    "        encoded = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        encoded = encoded.view(B, T, N, -1)\n",
    "        logits_time = self.time_head(encoded).squeeze(-1)\n",
    "        logits_rank = self.rank_delta_head(encoded).squeeze(-1)\n",
    "        return {\n",
    "            'lap_time': logits_time,\n",
    "            'rank_delta': logits_rank,\n",
    "        }\n",
    "\n",
    "class LapStateTransformer(nn.Module):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5898bc6",
   "metadata": {},
   "source": [
    "\n",
    "## Training utilities\n",
    "Multi-task losses (Huber for lap time, BCE for pit/DNF, cross entropy for compound) are masked by `alive_next` so retired or padded cars do not contribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89b02587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_time = nn.SmoothL1Loss(reduction='none')\n",
    "\n",
    "model = LapStateTransformer(\n",
    "    cfg=CONFIG,\n",
    "    vocab=vocab,\n",
    "    dyn_dim=train_dataset.sessions[0].dynamic_numeric.shape[-1],\n",
    "    global_dim=train_dataset.sessions[0].global_numeric.shape[-1],\n",
    "    static_dim=train_dataset.sessions[0].static_numeric.shape[-1],\n",
    ").to(TORCH_DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.learning_rate, weight_decay=CONFIG.weight_decay)\n",
    "\n",
    "\n",
    "def compute_losses(preds, batch):\n",
    "    targets = batch['targets']\n",
    "    alive_next = batch['alive_next'].to(TORCH_DEVICE)\n",
    "\n",
    "    time_pred = preds['lap_time']\n",
    "    time_tgt = targets['lap_time'].to(TORCH_DEVICE)\n",
    "    time_loss = loss_time(time_pred, time_tgt)\n",
    "    time_loss = (time_loss * alive_next).sum() / max(alive_next.sum().item(), 1.0)\n",
    "\n",
    "    rank_pred = preds['rank_delta']\n",
    "    rank_tgt = targets['rank_delta'].to(TORCH_DEVICE)\n",
    "    rank_loss = loss_time(rank_pred, rank_tgt)\n",
    "    rank_loss = (rank_loss * alive_next).sum() / max(alive_next.sum().item(), 1.0)\n",
    "\n",
    "    total = time_loss + rank_loss\n",
    "    metrics = {\n",
    "        'time_loss': time_loss.item(),\n",
    "        'rank_loss': rank_loss.item(),\n",
    "        'total': total.item(),\n",
    "    }\n",
    "    return total, metrics\n",
    "\n",
    "\n",
    "def run_epoch(loader, train: bool = True):\n",
    "    model.train() if train else model.eval()\n",
    "    epoch_metrics = []\n",
    "    steps = 0\n",
    "    context = tqdm(loader, desc='train' if train else 'val', leave=False)\n",
    "    for batch in context:\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        preds = model(batch)\n",
    "        loss, metrics = compute_losses(preds, batch)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        epoch_metrics.append(metrics)\n",
    "        context.set_postfix({k: f\"{v:.3f}\" for k, v in metrics.items()})\n",
    "        steps += 1\n",
    "        limit = CONFIG.max_train_steps_per_epoch if train else CONFIG.max_val_steps\n",
    "        if limit and steps >= limit:\n",
    "            break\n",
    "    if not epoch_metrics:\n",
    "        return {}\n",
    "    summary = {k: np.mean([m[k] for m in epoch_metrics]) for k in epoch_metrics[0]}\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f7f97",
   "metadata": {},
   "source": [
    "\n",
    "## Train for a few epochs\n",
    "For demonstration we cap the number of batches per epoch. Remove `max_train_steps_per_epoch`/`max_val_steps` to train on the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "332baef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train={'time_loss': 0.25353912541404705, 'pit_loss': 0.1262992090802496, 'compound_loss': 1.5984075872185535, 'dnf_loss': 0.0015930330103201872, 'total': 1.1162111235404373} | val={'time_loss': 0.2703601635041291, 'pit_loss': 0.15496818769392312, 'compound_loss': 1.3027927010859315, 'dnf_loss': 0.0, 'total': 0.9992406088858843}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_history = []\n",
    "val_history = []\n",
    "for epoch in range(1, CONFIG.num_epochs + 1):\n",
    "    train_metrics = run_epoch(train_loader, train=True)\n",
    "    val_metrics = run_epoch(val_loader, train=False)\n",
    "    train_history.append(train_metrics)\n",
    "    val_history.append(val_metrics)\n",
    "    print(f\"Epoch {epoch}: train={train_metrics} | val={val_metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620be47f",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation stub\n",
    "After training longer, evaluate on the held-out sessions and compute metrics such as lap-time MAE, pit F1, or end-of-race ranking accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58b4c060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation lap-time MAE (normalized units): 0.4934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_samples(loader, max_batches: int = 10):\n",
    "    model.eval()\n",
    "    all_time_errors = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            preds = model(batch)\n",
    "            time_pred = preds['lap_time']\n",
    "            time_tgt = batch['targets']['lap_time'].to(TORCH_DEVICE)\n",
    "            alive_next = batch['alive_next'].to(TORCH_DEVICE)\n",
    "            err = torch.abs(time_pred - time_tgt) * alive_next\n",
    "            denom = max(alive_next.sum().item(), 1.0)\n",
    "            all_time_errors.append(err.sum().item() / denom)\n",
    "            if max_batches and batch_idx + 1 >= max_batches:\n",
    "                break\n",
    "    return float(np.mean(all_time_errors)) if all_time_errors else float('nan')\n",
    "\n",
    "val_mae = evaluate_samples(val_loader, max_batches=CONFIG.max_val_steps)\n",
    "print(f\"Validation lap-time MAE (normalized units): {val_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a096fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d3c606c",
   "metadata": {},
   "source": [
    "\n",
    "## Save trained model weights\n",
    "Run this cell after training to persist the model (plus optimizer, config, and vocab metadata) for later reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f1adaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to /Users/ekazuki/Documents/f1stuff/models/models/lap_state_transformer.ckpt at 2025-12-07T18:46:05.257151Z\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = Path('models/lap_state_transformer.ckpt')\n",
    "checkpoint = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optimizer_state': optimizer.state_dict(),\n",
    "    'config': CONFIG.__dict__,\n",
    "    'vocab': {\n",
    "        'driver_to_idx': vocab.driver_to_idx,\n",
    "        'team_to_idx': vocab.team_to_idx,\n",
    "        'compound_to_idx': vocab.compound_to_idx,\n",
    "        'track_to_idx': vocab.track_to_idx,\n",
    "    },\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.utcnow().isoformat() + 'Z',\n",
    "        'train_history': train_history,\n",
    "        'val_history': val_history,\n",
    "    }\n",
    "}\n",
    "checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Saved checkpoint to {checkpoint_path.resolve()} at {checkpoint['metadata']['timestamp']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00cd76",
   "metadata": {},
   "source": [
    "\n",
    "## Generate a fictive race example\n",
    "The helper below takes one of the tensorized sessions, feeds sliding windows through the model, and constructs a synthetic ranking using the predicted lap times, pit probabilities, and DNF odds. It is a teacher-forced rollout (using the session context for inputs) but the leaderboard is based entirely on model outputs, so it behaves like a toy simulator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b42e14",
   "metadata": {},
   "source": [
    "\n",
    "## Race simulation helper\n",
    "This helper reuses the trained model to roll forward a window of laps, denormalizes the predicted lap times, and accumulates them into a synthetic leaderboard. The lap log records each driver's predicted lap time and running position for every generated lap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0326e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inverse_driver_vocab = {idx: name for name, idx in vocab.driver_to_idx.items()}\n",
    "inverse_track_vocab = {idx: name for name, idx in vocab.track_to_idx.items()}\n",
    "inverse_compound_vocab = {idx: name for name, idx in vocab.compound_to_idx.items()}\n",
    "\n",
    "def _session_window_batch(sess: SessionTensor, lap_start: int):\n",
    "    seq_slice = slice(lap_start, lap_start + CONFIG.seq_len)\n",
    "    return {\n",
    "        'dynamic': torch.from_numpy(sess.dynamic_numeric[seq_slice]).unsqueeze(0),\n",
    "        'global': torch.from_numpy(sess.global_numeric[seq_slice]).unsqueeze(0),\n",
    "        'compound_tokens': torch.from_numpy(sess.compound_tokens[seq_slice]).unsqueeze(0),\n",
    "        'rank_tokens': torch.from_numpy(sess.rank_tokens[seq_slice]).unsqueeze(0),\n",
    "        'alive_curr': torch.from_numpy(sess.alive_mask[seq_slice]).unsqueeze(0),\n",
    "        'alive_next': torch.from_numpy(sess.alive_mask[seq_slice]).unsqueeze(0),\n",
    "        'driver_tokens': torch.from_numpy(sess.driver_tokens).unsqueeze(0),\n",
    "        'team_tokens': torch.from_numpy(sess.team_tokens).unsqueeze(0),\n",
    "        'static_numeric': torch.from_numpy(sess.static_numeric).unsqueeze(0),\n",
    "        'track_token': torch.tensor([sess.track_token]),\n",
    "    }\n",
    "\n",
    "\n",
    "def _track_stats(track_token: int):\n",
    "    track_name = inverse_track_vocab.get(int(track_token), None)\n",
    "    if track_name and track_name in track_scalers:\n",
    "        stats = track_scalers[track_name]['lap_time_s']\n",
    "    else:\n",
    "        fallback = next(iter(track_scalers.keys()))\n",
    "        track_name = track_name or fallback\n",
    "        stats = track_scalers[fallback]['lap_time_s']\n",
    "    return track_name, stats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def simulate_fictive_race(sess: SessionTensor, laps_to_generate: int = 10, start_lap: int = 0):\n",
    "    model.eval()\n",
    "    track_name, (lap_mean, lap_std) = _track_stats(sess.track_token)\n",
    "    driver_names = [inverse_driver_vocab.get(int(tok), f'Driver_{idx:02d}') for idx, tok in enumerate(sess.driver_tokens) if tok != 0]\n",
    "    cumulative_times = {name: 0.0 for name in driver_names}\n",
    "    running_mask = {name: True for name in driver_names}\n",
    "    lap_rows = []\n",
    "\n",
    "    for step in range(laps_to_generate):\n",
    "        lap_start = start_lap + step\n",
    "        next_lap_idx = lap_start + CONFIG.seq_len\n",
    "        if next_lap_idx >= sess.dynamic_numeric.shape[0]:\n",
    "            break\n",
    "        batch = _session_window_batch(sess, lap_start)\n",
    "        preds = model(batch)\n",
    "        lap_pred = preds['lap_time'][0, -1].cpu().numpy()\n",
    "        pit_probs = torch.sigmoid(preds['pit'][0, -1]).cpu().numpy()\n",
    "        dnf_probs = torch.sigmoid(preds['dnf'][0, -1]).cpu().numpy()\n",
    "        compound_logits = torch.softmax(preds['compound'][0, -1], dim=-1).cpu().numpy()\n",
    "        lap_predictions = {}\n",
    "        for slot, driver_token in enumerate(sess.driver_tokens):\n",
    "            if driver_token == 0:\n",
    "                continue\n",
    "            driver_name = inverse_driver_vocab.get(int(driver_token), f'Driver_{slot:02d}')\n",
    "            if not running_mask[driver_name]:\n",
    "                continue\n",
    "            lap_seconds = float(lap_pred[slot] * lap_std + lap_mean)\n",
    "            cumulative_times[driver_name] += lap_seconds\n",
    "            lap_predictions[driver_name] = {\n",
    "                'pred_lap_time_s': lap_seconds,\n",
    "                'pit_probability': float(pit_probs[slot]),\n",
    "                'dnf_probability': float(dnf_probs[slot]),\n",
    "                'suggested_compound': inverse_compound_vocab.get(int(np.argmax(compound_logits[slot])), 'UNKNOWN'),\n",
    "            }\n",
    "            if lap_predictions[driver_name]['dnf_probability'] > 0.8:\n",
    "                running_mask[driver_name] = False\n",
    "        ordering = sorted(cumulative_times.items(), key=lambda kv: kv[1])\n",
    "        position_map = {name: idx + 1 for idx, (name, _) in enumerate(ordering)}\n",
    "        for driver_name, info in lap_predictions.items():\n",
    "            lap_rows.append({\n",
    "                'lap': next_lap_idx,\n",
    "                'driver': driver_name,\n",
    "                'race_position': position_map[driver_name],\n",
    "                'pred_lap_time_s': info['pred_lap_time_s'],\n",
    "                'pit_probability': info['pit_probability'],\n",
    "                'dnf_probability': info['dnf_probability'],\n",
    "                'suggested_compound': info['suggested_compound'],\n",
    "            })\n",
    "\n",
    "    leaderboard = pd.DataFrame([\n",
    "        {'driver': name, 'simulated_time_s': time, 'still_running': running_mask[name]}\n",
    "        for name, time in cumulative_times.items()\n",
    "    ]).sort_values('simulated_time_s').reset_index(drop=True)\n",
    "    lap_log = pd.DataFrame(lap_rows)\n",
    "    return track_name, leaderboard, lap_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19e5251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fictive race simulation on yas_island\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver</th>\n",
       "      <th>simulated_time_s</th>\n",
       "      <th>still_running</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAS</td>\n",
       "      <td>2806.351430</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAI</td>\n",
       "      <td>2806.351430</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAR</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RIC</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUL</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LEC</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OCO</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PIA</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VER</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ZHO</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NOR</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PER</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MAG</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HAM</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RUS</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BOT</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ALO</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>STR</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ALB</td>\n",
       "      <td>2806.351431</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TSU</td>\n",
       "      <td>2806.351432</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver  simulated_time_s  still_running\n",
       "0     GAS       2806.351430           True\n",
       "1     SAI       2806.351430           True\n",
       "2     SAR       2806.351431           True\n",
       "3     RIC       2806.351431           True\n",
       "4     HUL       2806.351431           True\n",
       "5     LEC       2806.351431           True\n",
       "6     OCO       2806.351431           True\n",
       "7     PIA       2806.351431           True\n",
       "8     VER       2806.351431           True\n",
       "9     ZHO       2806.351431           True\n",
       "10    NOR       2806.351431           True\n",
       "11    PER       2806.351431           True\n",
       "12    MAG       2806.351431           True\n",
       "13    HAM       2806.351431           True\n",
       "14    RUS       2806.351431           True\n",
       "15    BOT       2806.351431           True\n",
       "16    ALO       2806.351431           True\n",
       "17    STR       2806.351431           True\n",
       "18    ALB       2806.351431           True\n",
       "19    TSU       2806.351432           True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lap</th>\n",
       "      <th>driver</th>\n",
       "      <th>race_position</th>\n",
       "      <th>pred_lap_time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>ALB</td>\n",
       "      <td>1</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>ALO</td>\n",
       "      <td>2</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>BOT</td>\n",
       "      <td>3</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>GAS</td>\n",
       "      <td>4</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>HAM</td>\n",
       "      <td>5</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>HUL</td>\n",
       "      <td>6</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>LEC</td>\n",
       "      <td>7</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>MAG</td>\n",
       "      <td>8</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>NOR</td>\n",
       "      <td>9</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>OCO</td>\n",
       "      <td>10</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>PER</td>\n",
       "      <td>11</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>PIA</td>\n",
       "      <td>12</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>RIC</td>\n",
       "      <td>13</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>RUS</td>\n",
       "      <td>14</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>SAI</td>\n",
       "      <td>15</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>SAR</td>\n",
       "      <td>16</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>STR</td>\n",
       "      <td>17</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>TSU</td>\n",
       "      <td>18</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>VER</td>\n",
       "      <td>19</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>ZHO</td>\n",
       "      <td>20</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>ALB</td>\n",
       "      <td>3</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>ALO</td>\n",
       "      <td>4</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>BOT</td>\n",
       "      <td>5</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9</td>\n",
       "      <td>GAS</td>\n",
       "      <td>6</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9</td>\n",
       "      <td>HAM</td>\n",
       "      <td>7</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9</td>\n",
       "      <td>HUL</td>\n",
       "      <td>8</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9</td>\n",
       "      <td>LEC</td>\n",
       "      <td>9</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9</td>\n",
       "      <td>MAG</td>\n",
       "      <td>10</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>NOR</td>\n",
       "      <td>11</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>OCO</td>\n",
       "      <td>12</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9</td>\n",
       "      <td>PER</td>\n",
       "      <td>20</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9</td>\n",
       "      <td>PIA</td>\n",
       "      <td>13</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9</td>\n",
       "      <td>RIC</td>\n",
       "      <td>1</td>\n",
       "      <td>93.545047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>RUS</td>\n",
       "      <td>14</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>SAI</td>\n",
       "      <td>15</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9</td>\n",
       "      <td>SAR</td>\n",
       "      <td>16</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9</td>\n",
       "      <td>STR</td>\n",
       "      <td>17</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>9</td>\n",
       "      <td>TSU</td>\n",
       "      <td>18</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>VER</td>\n",
       "      <td>19</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>ZHO</td>\n",
       "      <td>2</td>\n",
       "      <td>93.545047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10</td>\n",
       "      <td>ALB</td>\n",
       "      <td>4</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10</td>\n",
       "      <td>ALO</td>\n",
       "      <td>5</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10</td>\n",
       "      <td>BOT</td>\n",
       "      <td>6</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10</td>\n",
       "      <td>GAS</td>\n",
       "      <td>1</td>\n",
       "      <td>93.545047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10</td>\n",
       "      <td>HAM</td>\n",
       "      <td>7</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>10</td>\n",
       "      <td>HUL</td>\n",
       "      <td>8</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10</td>\n",
       "      <td>LEC</td>\n",
       "      <td>9</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10</td>\n",
       "      <td>MAG</td>\n",
       "      <td>10</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>10</td>\n",
       "      <td>NOR</td>\n",
       "      <td>11</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10</td>\n",
       "      <td>OCO</td>\n",
       "      <td>12</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>10</td>\n",
       "      <td>PER</td>\n",
       "      <td>20</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>10</td>\n",
       "      <td>PIA</td>\n",
       "      <td>13</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10</td>\n",
       "      <td>RIC</td>\n",
       "      <td>2</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>10</td>\n",
       "      <td>RUS</td>\n",
       "      <td>14</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>10</td>\n",
       "      <td>SAI</td>\n",
       "      <td>15</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>10</td>\n",
       "      <td>SAR</td>\n",
       "      <td>16</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>10</td>\n",
       "      <td>STR</td>\n",
       "      <td>17</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>10</td>\n",
       "      <td>TSU</td>\n",
       "      <td>18</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>10</td>\n",
       "      <td>VER</td>\n",
       "      <td>19</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10</td>\n",
       "      <td>ZHO</td>\n",
       "      <td>3</td>\n",
       "      <td>93.545048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lap driver  race_position  pred_lap_time_s\n",
       "0     8    ALB              1        93.545048\n",
       "1     8    ALO              2        93.545048\n",
       "2     8    BOT              3        93.545048\n",
       "3     8    GAS              4        93.545048\n",
       "4     8    HAM              5        93.545048\n",
       "5     8    HUL              6        93.545048\n",
       "6     8    LEC              7        93.545048\n",
       "7     8    MAG              8        93.545048\n",
       "8     8    NOR              9        93.545048\n",
       "9     8    OCO             10        93.545048\n",
       "10    8    PER             11        93.545048\n",
       "11    8    PIA             12        93.545048\n",
       "12    8    RIC             13        93.545048\n",
       "13    8    RUS             14        93.545048\n",
       "14    8    SAI             15        93.545048\n",
       "15    8    SAR             16        93.545048\n",
       "16    8    STR             17        93.545048\n",
       "17    8    TSU             18        93.545048\n",
       "18    8    VER             19        93.545048\n",
       "19    8    ZHO             20        93.545048\n",
       "20    9    ALB              3        93.545048\n",
       "21    9    ALO              4        93.545048\n",
       "22    9    BOT              5        93.545048\n",
       "23    9    GAS              6        93.545048\n",
       "24    9    HAM              7        93.545048\n",
       "25    9    HUL              8        93.545048\n",
       "26    9    LEC              9        93.545048\n",
       "27    9    MAG             10        93.545048\n",
       "28    9    NOR             11        93.545048\n",
       "29    9    OCO             12        93.545048\n",
       "30    9    PER             20        93.545048\n",
       "31    9    PIA             13        93.545048\n",
       "32    9    RIC              1        93.545047\n",
       "33    9    RUS             14        93.545048\n",
       "34    9    SAI             15        93.545048\n",
       "35    9    SAR             16        93.545048\n",
       "36    9    STR             17        93.545048\n",
       "37    9    TSU             18        93.545048\n",
       "38    9    VER             19        93.545048\n",
       "39    9    ZHO              2        93.545047\n",
       "40   10    ALB              4        93.545048\n",
       "41   10    ALO              5        93.545048\n",
       "42   10    BOT              6        93.545048\n",
       "43   10    GAS              1        93.545047\n",
       "44   10    HAM              7        93.545048\n",
       "45   10    HUL              8        93.545048\n",
       "46   10    LEC              9        93.545048\n",
       "47   10    MAG             10        93.545048\n",
       "48   10    NOR             11        93.545048\n",
       "49   10    OCO             12        93.545048\n",
       "50   10    PER             20        93.545048\n",
       "51   10    PIA             13        93.545048\n",
       "52   10    RIC              2        93.545048\n",
       "53   10    RUS             14        93.545048\n",
       "54   10    SAI             15        93.545048\n",
       "55   10    SAR             16        93.545048\n",
       "56   10    STR             17        93.545048\n",
       "57   10    TSU             18        93.545048\n",
       "58   10    VER             19        93.545048\n",
       "59   10    ZHO              3        93.545048"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_session = next(sess for sess in session_tensors if sess.year in CONFIG.val_years)\n",
    "track_name, race_summary, lap_log = simulate_fictive_race(example_session, laps_to_generate=30, start_lap=0)\n",
    "print(f\"Fictive race simulation on {track_name}\")\n",
    "display(race_summary.head(20))\n",
    "display(lap_log[['lap', 'driver', 'race_position', 'pred_lap_time_s']].head(60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
